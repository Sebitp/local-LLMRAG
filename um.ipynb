{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmbm119/370RAG/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/nmbm119/370RAG/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: multiplying in assembly\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to\n",
    "# 1. define a query\n",
    "query = \"multiplying in assembly\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time take to get scores on 2670 embeddings: 0.00110 seconds.\n",
      "torch.return_types.topk(\n",
      "values=tensor([0.6931, 0.6652, 0.6646, 0.6575, 0.6566], device='cuda:0'),\n",
      "indices=tensor([643, 661, 638, 637, 634], device='cuda:0'))\n",
      "{'page_number': 268, 'chunk': 'A Multiply Algorithm Using 4-bit numbers to save space, multiply 2ten × 3ten, or 0010two × 0011two. Figure 3.6 shows the value of each register for each of the steps labeled  according to Figure 3.4, with the final value of 0000 0110two or 6ten. Color is  used to indicate the register values that change on that step, and the bit circled  is the one examined to determine the operation of the next step. EXAMPLE ANSWER Iteration Step Multiplier Multiplicand Product 0  Initial values 0011 0000 0010 0000 0000 1 1a: 1 ⇒ Prod = Prod + Mcand 0011 0000 0010 0000 0010 2: Shift left Multiplicand 0011 0000 0100 0000 0010 3: Shift right Multiplier 0001 0000 0100 0000 0010 2 1a: 1 ⇒ Prod = Prod + Mcand 0001 0000 0100 0000 0110 2: Shift left Multiplicand 0001 0000 1000 0000 0110 3: Shift right Multiplier 0000 0000 1000 0000 0110 3 1: 0 ⇒ No operation 0000 0000 1000 0000 0110 2: Shift left Multiplicand 0000 0001 0000 0000 0110 3: Shift right Multiplier 0000 0001 0000 0000 0110 4 1: 0 ⇒ No operation 0000 0001 0000 0000 0110 2: Shift left Multiplicand 0000 0010 0000 0000 0110 3: Shift right Multiplier 0000 0010 0000 0000 0110 FIGURE 3.6 Multiply example using algorithm in Figure 3.4. The bit examined to determine the  next step is circled in color.', 'chunk_char_count': 1250, 'chunk_word_count': 236, 'chunk_token_count': 312.5, 'embedding': array([-2.27601323e-02, -3.28163654e-02, -2.89165601e-02, -3.74529045e-03,\n",
      "       -8.31580684e-02,  3.89483906e-02,  4.58642989e-02, -2.40809154e-02,\n",
      "       -9.37492102e-02, -2.91597880e-02, -5.42682502e-03,  2.90957466e-02,\n",
      "        2.27728337e-02,  3.60048562e-02, -2.07407773e-02,  2.07204185e-02,\n",
      "        2.07421798e-02,  2.06687171e-02, -8.69641006e-02,  1.90357808e-02,\n",
      "        7.87848979e-03,  3.96948867e-03, -4.52733003e-02, -4.05130610e-02,\n",
      "       -2.99077341e-03, -2.46041641e-02, -5.37339561e-02,  1.64536014e-02,\n",
      "        2.04889979e-02,  1.76937804e-02, -5.55398017e-02,  9.11148451e-03,\n",
      "        1.59989186e-02,  1.12969019e-02,  1.99272722e-06, -1.85556933e-02,\n",
      "       -3.75827588e-02,  2.45447475e-02,  1.11283548e-02,  9.33192521e-02,\n",
      "        4.15448025e-02,  9.39729661e-02, -1.52995382e-02,  4.56312448e-02,\n",
      "       -9.18917730e-03, -2.78415289e-02,  1.16209872e-02,  2.62097623e-02,\n",
      "        1.60854086e-02,  2.78739799e-02, -1.80118461e-03, -4.73316759e-02,\n",
      "        2.82783266e-02, -1.17863948e-03,  7.08975364e-03, -6.31029457e-02,\n",
      "        6.34002984e-02,  3.58798541e-02,  1.85579769e-02,  2.30393801e-02,\n",
      "        2.53210850e-02, -2.98755318e-02, -1.10155344e-03, -1.31891519e-02,\n",
      "       -7.00385310e-03, -2.85273790e-02,  7.07454085e-02, -4.29476388e-02,\n",
      "       -3.28268632e-02,  4.33274880e-02, -4.25294181e-03, -4.90979962e-02,\n",
      "        2.33723293e-03,  9.78049915e-03,  1.44146811e-02, -6.96698353e-02,\n",
      "       -4.11417969e-02, -3.33471745e-02,  1.76832415e-02, -1.52335493e-02,\n",
      "       -3.80241498e-02, -3.83168422e-02, -1.29010901e-02, -2.78307963e-02,\n",
      "       -7.97159038e-03,  6.46594167e-02,  7.99337681e-03, -3.01124491e-02,\n",
      "       -1.74128767e-02, -2.66240872e-02, -4.34217490e-02,  7.19801942e-03,\n",
      "        3.04531325e-02,  4.19473387e-02, -4.36573988e-03,  8.83606635e-03,\n",
      "       -2.93378755e-02, -3.13142873e-02, -2.68062064e-03, -7.16385841e-02,\n",
      "        4.71880147e-03,  4.11936902e-02, -1.51947672e-02,  4.98641282e-02,\n",
      "       -2.21938249e-02,  5.64871263e-03, -8.79720896e-02,  2.23836931e-03,\n",
      "       -5.48320636e-02,  8.24098438e-02,  4.62674983e-02, -2.31315033e-03,\n",
      "       -4.30197306e-02, -4.39655967e-03,  5.32824844e-02, -2.44990624e-02,\n",
      "       -7.46920854e-02, -1.33674666e-02, -3.34995650e-02,  2.72052716e-02,\n",
      "        5.20644384e-03, -1.69005450e-02,  7.92124495e-02,  6.70976844e-03,\n",
      "       -4.84385155e-03, -1.02763521e-02, -1.36443973e-02, -3.55808139e-02,\n",
      "       -2.17560846e-02,  1.62922442e-02, -9.23738990e-04,  2.19959840e-02,\n",
      "       -4.82148305e-03,  3.56984027e-02, -4.01798561e-02,  1.43750862e-04,\n",
      "        3.19037400e-02,  4.40490358e-02, -1.87098235e-02, -3.33958282e-03,\n",
      "        3.62212360e-02,  4.40178253e-03,  3.87849510e-02,  6.44294219e-03,\n",
      "       -9.87882633e-03,  2.89922627e-03, -1.04494868e-02,  5.59153445e-02,\n",
      "        1.78380553e-02,  5.41502833e-02,  2.57738214e-02,  3.49690430e-02,\n",
      "       -4.20605168e-02, -3.33718397e-02,  1.01271011e-02,  1.55495647e-02,\n",
      "       -9.19955876e-03, -3.30729294e-03, -5.40908892e-03, -1.32946223e-02,\n",
      "       -2.50726957e-02, -2.37654168e-02, -4.70528472e-03, -2.53335442e-02,\n",
      "        7.74422195e-03, -5.66678606e-02, -3.44038233e-02, -1.25755901e-02,\n",
      "       -2.93090530e-02,  1.71461664e-02,  1.52229443e-02, -1.40552307e-02,\n",
      "       -4.97156344e-02,  1.67161692e-02, -6.46486878e-02, -1.79594364e-02,\n",
      "        3.11792921e-02,  7.78318420e-02,  6.39394969e-02, -1.84675306e-03,\n",
      "       -2.56595425e-02, -3.29234428e-03, -8.15907551e-04,  6.50425851e-02,\n",
      "       -4.03204076e-02, -6.44459873e-02, -1.78168714e-02, -1.42389154e-02,\n",
      "       -2.78138090e-02,  2.29241955e-03,  5.74443452e-02,  1.72512587e-02,\n",
      "        2.51206383e-02, -6.73579564e-03, -5.22572249e-02, -8.71264085e-04,\n",
      "       -1.80039741e-02,  7.68432627e-03,  3.48073952e-02, -2.52235513e-02,\n",
      "       -2.63581388e-02, -2.81839911e-02,  3.69169563e-03, -1.63208749e-02,\n",
      "        1.63054168e-02, -1.91313978e-02,  6.66635856e-02,  1.43506434e-02,\n",
      "       -4.72660251e-02, -6.03298247e-02,  5.84214600e-03,  1.05307251e-02,\n",
      "       -5.41676916e-02, -5.39767928e-02, -4.21171635e-02,  2.37216540e-02,\n",
      "       -2.74101626e-02,  4.95233946e-02, -9.63030569e-03, -2.94499192e-02,\n",
      "        2.26187035e-02,  3.59397493e-02,  7.48393461e-02, -5.29321022e-02,\n",
      "        2.03636126e-03, -1.41216880e-02,  3.01073529e-02,  3.38962674e-02,\n",
      "       -4.80155312e-02,  4.42651585e-02,  3.66160236e-02,  1.37233324e-02,\n",
      "        4.01877388e-02,  2.43865661e-02, -8.82268399e-02,  1.86811155e-03,\n",
      "       -6.27989555e-03, -7.72379246e-03,  3.75104398e-02,  4.50192811e-03,\n",
      "       -1.65535063e-02, -3.47482902e-03,  1.17967566e-02, -1.99856167e-03,\n",
      "        9.49415565e-02, -3.22365351e-02, -9.60919634e-03, -4.30268124e-02,\n",
      "        4.59420532e-02, -2.16065142e-02, -3.03902719e-02,  3.61900888e-02,\n",
      "        7.90802687e-02, -1.48929600e-02,  5.90676069e-02,  5.47315627e-02,\n",
      "       -9.43540223e-03, -2.07287334e-02,  3.03717703e-02,  7.64182359e-02,\n",
      "       -3.02147064e-02, -3.78257558e-02, -2.99972505e-03, -4.62453663e-02,\n",
      "       -1.16626564e-02,  1.43878879e-05, -2.01100297e-02,  1.54233780e-02,\n",
      "       -2.71226801e-02, -4.87261266e-02,  2.94322912e-02,  1.34892743e-02,\n",
      "        1.53664546e-02,  6.54916316e-02, -1.50480354e-02, -3.11327111e-02,\n",
      "        3.37022543e-02, -7.29953721e-02, -3.82679366e-02, -2.56508850e-02,\n",
      "        3.98574919e-02, -4.50487696e-02,  3.73550169e-02, -7.42266700e-03,\n",
      "        5.75069105e-03,  2.71345116e-02, -3.13485451e-02,  2.43884772e-02,\n",
      "       -9.32286531e-02, -2.40598749e-02,  3.89936753e-03, -1.77080370e-02,\n",
      "       -8.65590479e-03,  4.59834933e-02, -5.89882098e-02, -2.13580821e-02,\n",
      "        9.44947544e-03, -8.38166010e-03,  9.07161925e-03,  4.09345925e-02,\n",
      "       -3.54317180e-03,  8.50101486e-02,  4.08494053e-03,  1.76519603e-02,\n",
      "        4.86224890e-02,  2.27121543e-03,  5.34035116e-02,  3.03710476e-02,\n",
      "       -2.83626020e-02,  2.68525258e-02,  2.46465532e-03,  1.22166760e-02,\n",
      "        1.70211587e-02, -5.19940220e-02,  5.45554720e-02,  3.29173217e-03,\n",
      "       -2.66097742e-03, -3.16376165e-02, -3.95207778e-02,  3.34073137e-03,\n",
      "        2.07277085e-03,  1.74398702e-02, -1.73844192e-02, -1.54261105e-03,\n",
      "       -1.04087638e-02, -2.14761961e-02, -1.68456498e-03,  5.61164878e-02,\n",
      "       -9.62153263e-03, -2.13363394e-02,  4.99575101e-02, -5.61837740e-02,\n",
      "       -2.25308798e-02,  1.06296249e-01,  6.42257929e-02, -1.06094044e-03,\n",
      "       -6.64376542e-02, -2.17156634e-02,  2.45209802e-02, -3.91595764e-03,\n",
      "        1.05941156e-03,  1.39917284e-02,  1.88048519e-02,  1.44003397e-02,\n",
      "        6.55049682e-02,  1.37885744e-02,  4.13977094e-02, -4.90261847e-03,\n",
      "       -7.82178435e-03, -1.71933658e-02,  1.03182040e-01, -3.73854488e-02,\n",
      "       -2.41898969e-02, -9.57539398e-03, -1.49802007e-02,  2.99142208e-03,\n",
      "        8.75133555e-03, -2.56991778e-02, -1.98344886e-02,  1.59891192e-02,\n",
      "        7.69088161e-04, -1.38869975e-04,  2.40493193e-02,  8.77076387e-02,\n",
      "       -8.26962963e-02, -5.31331543e-03, -5.87218674e-03, -1.97205618e-02,\n",
      "        1.24364393e-02,  2.56587453e-02, -9.48421564e-03,  5.90471961e-02,\n",
      "        2.91879550e-02, -2.77586561e-02, -9.29693878e-03, -5.86092621e-02,\n",
      "        3.78258689e-03, -1.49083836e-02, -5.09657301e-02,  4.02411819e-02,\n",
      "       -3.11425366e-02, -8.63210931e-02,  2.08836850e-02, -2.34100688e-02,\n",
      "       -6.03879392e-02, -1.98067315e-02,  2.74455786e-04, -3.73072959e-02,\n",
      "       -3.62433083e-02, -3.75939868e-02, -2.02185046e-02, -3.87011953e-02,\n",
      "       -6.32976294e-02, -6.53248504e-02,  3.84958670e-03,  3.89592834e-02,\n",
      "        2.16788389e-02,  6.20452389e-02, -1.08270515e-02, -7.02040223e-03,\n",
      "       -3.33461426e-02,  5.58210574e-02,  2.42132135e-02,  8.98002298e-04,\n",
      "        2.33307872e-02, -2.79084966e-02,  3.25932167e-02,  9.62394569e-03,\n",
      "       -2.31923256e-02, -3.51917930e-02,  3.44883166e-02, -9.40995477e-03,\n",
      "        4.80843037e-02, -4.77606580e-02,  1.63797638e-03,  3.03318184e-02,\n",
      "        1.18986573e-02, -1.43648358e-02,  4.98045497e-02, -1.42194983e-02,\n",
      "        4.11663577e-02,  3.80803179e-03,  1.00886999e-02,  5.28291166e-02,\n",
      "        3.59056378e-03,  1.19954096e-02,  5.14462963e-02,  4.52698991e-02,\n",
      "       -3.28995213e-02,  2.85952371e-02,  3.06847296e-03, -2.91777458e-02,\n",
      "       -4.03578430e-02,  4.18295749e-02, -8.51426274e-02, -1.50369825e-02,\n",
      "       -3.93475555e-02,  1.73861403e-02,  2.69429050e-02,  6.83814883e-02,\n",
      "        4.44140937e-03,  2.66664498e-03,  4.08267193e-02,  1.05241984e-02,\n",
      "       -6.33158684e-02,  7.05735758e-02, -7.22723594e-03, -5.33747524e-02,\n",
      "       -7.25013912e-02,  4.62197822e-05,  6.26308769e-02,  1.49564128e-02,\n",
      "        2.99814269e-02,  6.38390309e-04,  2.96563767e-02,  1.32409204e-02,\n",
      "       -4.14656773e-02, -8.13828856e-02,  2.14764872e-03, -6.09097369e-02,\n",
      "        2.45573875e-02,  1.06641054e-02, -3.85431536e-02, -1.31995771e-02,\n",
      "        2.55342443e-02, -5.69425113e-02, -4.99937497e-02, -3.13186250e-03,\n",
      "        3.49847637e-02, -3.50718349e-02, -2.07201112e-02, -2.78792018e-03,\n",
      "       -1.27909370e-02, -1.08322343e-02,  1.63898692e-02,  2.71135904e-02,\n",
      "        5.41594736e-02, -5.28978221e-02, -4.53854998e-04, -4.65156371e-03,\n",
      "       -2.31348835e-02, -1.17002847e-02,  1.24860378e-02,  4.95026074e-02,\n",
      "        2.12862790e-02,  3.92838567e-02, -2.68418975e-02,  2.01807115e-02,\n",
      "       -2.58991681e-02,  3.65808457e-02,  3.32357585e-02, -8.15951750e-02,\n",
      "        4.69647115e-03, -3.70414853e-02,  7.16540217e-03, -4.33709435e-02,\n",
      "        2.28686333e-02,  1.99081711e-02, -1.48059456e-02,  3.34372111e-02,\n",
      "       -1.64214373e-02, -1.39171435e-02, -2.73879822e-02,  9.69376881e-03,\n",
      "        5.84595278e-02,  1.45295421e-02, -5.72990975e-04,  4.72055487e-02,\n",
      "       -1.01226699e-02,  4.63068634e-02,  1.18626058e-02, -2.47884821e-03,\n",
      "        4.08781506e-02, -3.95461880e-02,  4.51708212e-02, -5.05538210e-02,\n",
      "        4.37277071e-02,  5.34107871e-02,  7.11730048e-02,  3.43108061e-03,\n",
      "        2.75466982e-02, -4.70769331e-02,  7.72464871e-02, -7.04321414e-02,\n",
      "       -7.66885933e-03, -4.90168817e-02,  3.61845791e-02, -3.17565985e-02,\n",
      "       -4.43773083e-02,  7.42996251e-03, -1.81763470e-02, -1.84156317e-02,\n",
      "       -3.52357589e-02,  3.47496904e-02, -2.22967360e-02, -5.58473133e-02,\n",
      "        3.25060077e-02,  4.26835790e-02,  2.90224259e-03, -1.84551498e-03,\n",
      "        2.72351187e-02,  3.81925255e-02, -6.79184869e-02,  2.13687564e-03,\n",
      "        6.95825815e-02, -1.69805046e-02, -4.14817082e-03,  8.86336900e-03,\n",
      "       -3.82290035e-02,  2.77492916e-03, -6.51345635e-03,  1.18355537e-02,\n",
      "        3.57695259e-02, -5.01361527e-02, -1.15621742e-02, -4.93044098e-33,\n",
      "       -1.19999815e-02,  2.20863335e-03,  2.09291782e-02,  3.62811387e-02,\n",
      "       -1.68378651e-02,  3.00895772e-03,  4.89032045e-02,  4.56876121e-02,\n",
      "       -7.87724555e-03, -4.43673953e-02,  2.96739619e-02, -3.03905718e-02,\n",
      "        2.97887105e-04, -2.96972133e-02,  6.03887485e-03,  5.17199673e-02,\n",
      "        4.69581736e-03, -5.61460154e-03,  5.68430405e-03,  3.16209085e-02,\n",
      "       -3.35410871e-02, -6.99670753e-03,  4.10819612e-02,  7.69793615e-02,\n",
      "       -9.69972461e-03,  2.20739823e-02, -1.82378329e-02,  1.47340568e-02,\n",
      "       -4.67970036e-02, -2.95278039e-02,  3.15806568e-02,  2.62650487e-04,\n",
      "       -5.43762976e-03, -2.83309370e-02,  1.05389170e-02,  1.56212309e-02,\n",
      "       -7.11152479e-02, -2.54737977e-02,  1.55366408e-02,  2.88702808e-02,\n",
      "       -6.10366650e-02,  2.51194630e-02, -1.65800005e-02,  5.51785119e-02,\n",
      "       -2.78499350e-02,  1.97597425e-02, -2.21444983e-02,  3.50029506e-02,\n",
      "       -2.05657934e-03, -1.25012230e-02, -2.61425530e-03,  4.12351303e-02,\n",
      "        3.38163110e-03,  6.55182230e-05,  4.39072251e-02, -2.48402301e-02,\n",
      "       -4.98563312e-02, -9.60024968e-02, -4.87916879e-02,  3.74328159e-02,\n",
      "        5.10021299e-02,  4.89520933e-03, -1.85403954e-02,  1.48620782e-03,\n",
      "        1.18458979e-02, -4.93056476e-02,  4.65516262e-02,  3.49475145e-02,\n",
      "        1.01357186e-02,  1.66326463e-02, -2.87558641e-02,  3.22594494e-02,\n",
      "       -4.30876128e-02, -2.53572762e-02,  9.29651037e-02,  2.23064050e-03,\n",
      "        2.46152207e-02,  5.20438813e-02,  5.84582724e-02, -3.57325114e-02,\n",
      "        1.72262415e-02, -3.28685269e-02,  3.52652371e-03, -2.94874376e-03,\n",
      "        2.57283133e-02,  5.71630895e-02, -6.55783415e-02, -1.03748739e-02,\n",
      "       -3.05587705e-02,  1.83213584e-03, -1.48150837e-02,  1.12149730e-01,\n",
      "       -3.99468914e-02,  2.73695569e-02, -5.55245159e-03,  9.38185025e-03,\n",
      "        4.02866267e-02,  7.43878353e-03, -2.54102983e-03, -3.25746387e-02,\n",
      "        3.19568776e-02, -6.04195055e-04, -1.78173855e-02, -3.00133713e-02,\n",
      "        4.33619060e-02, -3.04576039e-04, -4.36541215e-02,  5.32262400e-02,\n",
      "       -3.36260386e-02, -2.01630779e-02, -9.06129181e-03, -4.22359742e-02,\n",
      "       -1.69620868e-02, -6.61699474e-02, -1.01910792e-02, -4.28735465e-03,\n",
      "       -1.33241387e-02, -1.57546846e-03, -6.91450841e-04, -5.88565767e-02,\n",
      "       -2.80427327e-03, -3.29589024e-02, -1.05960276e-02,  5.96449301e-02,\n",
      "       -9.07324329e-02, -2.58630980e-02,  1.81055311e-02, -4.65873769e-03,\n",
      "        1.07791945e-02, -2.61016656e-02, -5.74212447e-02,  3.40111814e-02,\n",
      "        2.58803027e-07,  1.80069357e-02,  2.67065428e-02,  4.17836718e-02,\n",
      "        5.34066074e-02,  4.58210036e-02, -7.18110707e-04,  2.33152173e-02,\n",
      "        5.89734651e-02, -2.18207631e-02, -6.28628805e-02, -1.79144889e-02,\n",
      "       -4.45810612e-03, -3.69263105e-02, -6.30512312e-02,  1.10387892e-01,\n",
      "       -8.73872172e-03,  3.73110175e-02,  3.99621353e-02,  2.34361589e-02,\n",
      "        3.04646119e-02,  7.71364793e-02, -1.54211940e-02,  1.22609902e-02,\n",
      "        8.19505844e-03,  3.58210541e-02,  5.77735994e-03,  2.40326654e-02,\n",
      "        2.71462034e-02,  4.78345901e-02, -3.28053534e-02, -6.79520294e-02,\n",
      "       -1.63000338e-02, -1.51160080e-02, -5.04346155e-02,  1.45842209e-02,\n",
      "       -6.35904958e-03, -2.52640573e-03,  1.06790625e-01, -1.85761619e-02,\n",
      "       -3.42944404e-03,  1.92150753e-02, -1.98762622e-02, -3.55914943e-02,\n",
      "       -1.01836463e-02, -1.96025316e-02,  3.95127051e-02, -7.36659486e-03,\n",
      "       -4.27980796e-02,  1.56975836e-02, -1.93568431e-02, -7.94881675e-03,\n",
      "        1.66714191e-02,  5.36264628e-02,  1.35753760e-02,  1.80410594e-02,\n",
      "       -1.02390302e-02,  9.02694417e-04,  2.00018194e-02,  2.36655306e-03,\n",
      "        2.40692813e-02, -6.38967380e-02, -1.90743059e-02,  3.09934579e-02,\n",
      "       -1.06697604e-02, -1.36939459e-03,  5.27318589e-05, -1.27914557e-02,\n",
      "        2.46246299e-34,  3.83176878e-02, -2.20119040e-02,  2.54051611e-02,\n",
      "        2.44790781e-02, -2.10703574e-02, -1.52833601e-02, -1.45396059e-02,\n",
      "       -4.51380312e-02, -4.77339290e-02,  3.65707204e-02, -5.73959798e-02])}\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "print(top_results_dot_product)\n",
    "print(pages_and_chunks[643])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'multiplying in assembly'\n",
      "\n",
      "Results:\n",
      "Score: 0.6931\n",
      "Text:\n",
      "A Multiply Algorithm Using 4-bit numbers to save space, multiply 2ten × 3ten, or\n",
      "0010two × 0011two. Figure 3.6 shows the value of each register for each of the\n",
      "steps labeled  according to Figure 3.4, with the final value of 0000 0110two or\n",
      "6ten. Color is  used to indicate the register values that change on that step,\n",
      "and the bit circled  is the one examined to determine the operation of the next\n",
      "step. EXAMPLE ANSWER Iteration Step Multiplier Multiplicand Product 0  Initial\n",
      "values 0011 0000 0010 0000 0000 1 1a: 1 ⇒ Prod = Prod + Mcand 0011 0000 0010\n",
      "0000 0010 2: Shift left Multiplicand 0011 0000 0100 0000 0010 3: Shift right\n",
      "Multiplier 0001 0000 0100 0000 0010 2 1a: 1 ⇒ Prod = Prod + Mcand 0001 0000 0100\n",
      "0000 0110 2: Shift left Multiplicand 0001 0000 1000 0000 0110 3: Shift right\n",
      "Multiplier 0000 0000 1000 0000 0110 3 1: 0 ⇒ No operation 0000 0000 1000 0000\n",
      "0110 2: Shift left Multiplicand 0000 0001 0000 0000 0110 3: Shift right\n",
      "Multiplier 0000 0001 0000 0000 0110 4 1: 0 ⇒ No operation 0000 0001 0000 0000\n",
      "0110 2: Shift left Multiplicand 0000 0010 0000 0000 0110 3: Shift right\n",
      "Multiplier 0000 0010 0000 0000 0110 FIGURE 3.6 Multiply example using algorithm\n",
      "in Figure 3.4. The bit examined to determine the  next step is circled in color.\n",
      "Page number: 268\n",
      "\n",
      "\n",
      "Score: 0.6652\n",
      "Text:\n",
      "LEGv8 assembly language Category  Instruction Example Meaning Comments\n",
      "Arithmetic add  ADD    X1, X2, X3 SUB    X1, X2, X3 ADDI   X1, X2, 20 SUBI   X1,\n",
      "X2, 20 ADDS   X1, X2, X3 SUBS   X1, X2, X3 ADDIS  X1, X2, 20 SUBIS  X1, X2, 20\n",
      "LDUR   X1, [X2,40] X1 = X2 + X3 X1 = X2 – X3 X1 = X2 + 20 X1 = X2 – 20 X1 = X2 +\n",
      "X3 X1 = X2 – X3 X1 = X2 + 20 X1 = X2 – 20 Subtract constant, set condition codes\n",
      "Three register operands subtract Three register operands add immediate Used to\n",
      "add constants subtract immediate Used to subtract constants add and set flags\n",
      "Add, set condition codes subtract and set flags Subtract, set condition codes\n",
      "add immediate and set  flags Add constant, set condition codes subtract\n",
      "immediate and  set flags multiply MUL    X1, X2, X3 X1 = X2 × X3 Lower 64-bits\n",
      "of 128-bit product signed multiply high SMULH  X1, X2, X3 X1 = X2 × X3 Upper\n",
      "64-bits of 128-bit signed product unsigned multiply high UMULH  X1, X2, X3 X1 =\n",
      "X2 × X3 Upper 64-bits of 128-bit unsigned product signed divide SDIV   X1, X2,\n",
      "X3 X1 = X2 / X3 Divide, treating operands as signed unsigned divide UDIV   X1,\n",
      "X2, X3 X1 = X2 / X3 Divide, treating operands as unsigned Data transfer  load\n",
      "register Doubleword from memory to register store register STUR   X1, [X2,40]\n",
      "LDURSW X1, [X2,40] Doubleword from register to memory load signed word Word from\n",
      "memory to register store word STURW  X1, [X2,40] LDURH  X1, [X2,40] Word from\n",
      "register to memory  load half Halfword memory to register store half STURH  X1,\n",
      "[X2,40] LDURB  X1, [X2,40] Halfword register to memory load byte Byte from\n",
      "memory to register store byte STURB  X1, [X2,40] LDXR   X1, [X2,0] STXR   X1,\n",
      "X3, [X2] Byte from register to memory load exclusive register Load; 1st half of\n",
      "atomic swap store exclusive register Store; 2nd half of atomic swap move wide\n",
      "with zero Loads 16-bit constant, rest zeros  MOVZ   X1,20 X1 = Memory[X2 + 40]\n",
      "Memory[X2 + 40] = X1 X1 = Memory[X2 + 40] Memory[X2 + 40] = X1 X1 = Memory[X2 +\n",
      "40] Memory[X2 + 40] = X1 X1 = Memory[X2 + 40] Memory[X2 + 40] = X1 X1 =\n",
      "Memory[X2] Memory[X2]=X1;X3=0 or 1 X1 = 20 or 20 * 216 or 20 * 232 or 20 * 248\n",
      "Logical and Three reg. operands; bit-by-bit AND inclusive or Three reg.\n",
      "operands; bit-by-bit OR exclusive or Three reg. operands; bit-by-bit XOR and\n",
      "immediate Bit-by-bit AND reg with constant inclusive or immediate Bit-by-bit OR\n",
      "reg with constant exclusive or immediate Bit-by-bit XOR reg with constant\n",
      "logical shift left Shift left by constant Condi- tional  branch compare and\n",
      "branch on equal 0 if (X1 == 0) go to PC +  4 + 100 Equal 0 test; PC-relative\n",
      "branch branch conditionally B.cond 25 if (condition true) go to PC + 4 + 100\n",
      "Test condition codes; if true, branch Uncondi- tional    jump branch B      2500\n",
      "go to PC + 4 + 10000 Branch to target address; PC-relative branch to register BR\n",
      "X30 go to X30 For switch, procedure return branch with link BL     2500 X30 = PC\n",
      "+ 4; PC + 4 + 10000 For procedure call PC-relative logical shift right AND\n",
      "X1, X2, X3 ORR    X1, X2, X3 EOR    X1, X2, X3 ANDI   X1, X2, 20 ORRI   X1, X2,\n",
      "20 EORI   X1, X2, 20 LSL    X1, X2, 10 LSR    X1, X2, 10 CBZ    X1, 25 compare\n",
      "and branch on  not equal 0 if (X1!= 0) go to PC +  4 + 100 Not equal 0 test; PC-\n",
      "relative CBNZ   X1, 25 X1 = X2 & X3 X1 = X2 | X3 X1 = X2 ^ X3 X1 = X2 & 20 X1 =\n",
      "X2 | 20 X1 = X2 ^ 20 X1 = X2 << 10 X1 = X2 >> 10 Shift right by constant move\n",
      "wide with keep Loads 16-bit constant, rest unchanged MOVK   X1,20 X1 = 20 or 20\n",
      "* 216 or 20 * 232 or 20 * 248 FIGURE 3.12 LEGv8 core architecture. LEGv8 machine\n",
      "language is listed in the LEGv8 Reference Data Card at the front of this book.\n",
      "Page number: 277\n",
      "\n",
      "\n",
      "Score: 0.6646\n",
      "Text:\n",
      "The least significant  bit of the multiplier (Multiplier0) determines whether\n",
      "the multiplicand is added to  the Product register. The left shift in step 2 has\n",
      "the effect of moving the intermediate  operands to the left, just as when\n",
      "multiplying with paper and pencil. The shift right  in step 3 gives us the next\n",
      "bit of the multiplier to examine in the following iteration.  These three steps\n",
      "are repeated 64 times to obtain the product. If each step took a  clock cycle,\n",
      "this algorithm would require almost 200 clock cycles to multiply two  64-bit\n",
      "numbers. The relative importance of arithmetic operations like multiply  varies\n",
      "with the program, but addition and subtraction may be anywhere from 5 to  100\n",
      "times more popular than multiply. Accordingly, in many applications, multiply\n",
      "can take several clock cycles without significantly affecting performance.\n",
      "However,  Amdahl’s Law (see Section 1.10) reminds us that even a moderate\n",
      "frequency for a  slow operation can limit performance.\n",
      "Page number: 266\n",
      "\n",
      "\n",
      "Score: 0.6575\n",
      "Text:\n",
      "3.3 Multiplication  193 Multiplicand Shift left 128 bits 128-bit ALU Product\n",
      "Write 128 bits Control test Multiplier Shift right 64 bits FIGURE 3.3 First\n",
      "version of the multiplication hardware. The Multiplicand register, ALU, and\n",
      "Product register are all 128 bits wide, with only the Multiplier register\n",
      "containing 64 bits. ( Appendix A  describes ALUs.) The 64-bit multiplicand\n",
      "starts in the right half of the Multiplicand register and is shifted left  1 bit\n",
      "on each step. The multiplier is shifted in the opposite direction at each step.\n",
      "The algorithm starts with  the product initialized to 0. Control decides when to\n",
      "shift the Multiplicand and Multiplier registers and when  to write new values\n",
      "into the Product register. Figure 3.4 shows the three basic steps needed for\n",
      "each bit.\n",
      "Page number: 266\n",
      "\n",
      "\n",
      "Score: 0.6566\n",
      "Text:\n",
      "192  Chapter 3 Arithmetic for Computers The first operand is called the\n",
      "multiplicand and the second the multiplier.  The final result is called the\n",
      "product. As you may recall, the algorithm learned  in grammar school is to take\n",
      "the digits of the multiplier one at a time from right  to left, multiplying the\n",
      "multiplicand by the single digit of the multiplier, and  shifting the\n",
      "intermediate product one digit to the left of the earlier intermediate\n",
      "products. The first observation is that the number of digits in the product is\n",
      "considerably  larger than the number in either the multiplicand or the\n",
      "multiplier. In fact, if we  ignore the sign bits, the length of the\n",
      "multiplication of an n-bit multiplicand and an  m-bit multiplier is a product\n",
      "that is n + m bits long. That is, n + m bits are required  to represent all\n",
      "possible products. Hence, like add, multiply must cope with  overflow because we\n",
      "frequently want a 64-bit product as the result of multiplying  two 64-bit\n",
      "numbers. In this example, we restricted the decimal digits to 0 and 1.\n",
      "Page number: 265\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 2670 embeddings: 0.00007 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.7239, 0.6988, 0.6887, 0.6813, 0.6799], device='cuda:0'),\n",
       " tensor([638, 168, 635, 645, 808], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"efficient multiplication\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 2670 embeddings: 0.00013 seconds.\n",
      "Query: efficient multiplication\n",
      "\n",
      "Results:\n",
      "Score: 0.7239\n",
      "The least significant  bit of the multiplier (Multiplier0) determines whether\n",
      "the multiplicand is added to  the Product register. The left shift in step 2 has\n",
      "the effect of moving the intermediate  operands to the left, just as when\n",
      "multiplying with paper and pencil. The shift right  in step 3 gives us the next\n",
      "bit of the multiplier to examine in the following iteration.  These three steps\n",
      "are repeated 64 times to obtain the product. If each step took a  clock cycle,\n",
      "this algorithm would require almost 200 clock cycles to multiply two  64-bit\n",
      "numbers. The relative importance of arithmetic operations like multiply  varies\n",
      "with the program, but addition and subtraction may be anywhere from 5 to  100\n",
      "times more popular than multiply. Accordingly, in many applications, multiply\n",
      "can take several clock cycles without significantly affecting performance.\n",
      "However,  Amdahl’s Law (see Section 1.10) reminds us that even a moderate\n",
      "frequency for a  slow operation can limit performance.\n",
      "Page number: 266\n",
      "\n",
      "\n",
      "Score: 0.6988\n",
      "How much do I have to improve the speed of multiplication if I want my  program\n",
      "to run five times faster? The execution time of the program after making the\n",
      "improvement is given by  the following simple equation known as Amdahl’s Law:\n",
      "Executiontimeaffer improvement Executiontimeafected by improvement Amount of\n",
      "improvement Executiontimeunafected For this problem: Execution\n",
      "timeaferimprovement seconds seconds 80 100 80 n ( ) Amdahl’s Law   A rule\n",
      "stating that  the performance  enhancement possible  with a given improvement\n",
      "is limited by the amount  that the improved feature  is used. It is a\n",
      "quantitative  version of the law of  diminishing returns. Science must begin\n",
      "with myths, and the  criticism of myths. Sir Karl Popper, The  Philosophy of\n",
      "Science,  1957\n",
      "Page number: 73\n",
      "\n",
      "\n",
      "Score: 0.6887\n",
      "With only two  choices, each step of the multiplication is simple: 1. Just place\n",
      "a copy of the multiplicand (1 × multiplicand) in the proper place  if the\n",
      "multiplier digit is a 1, or 2. Place 0 (0 × multiplicand) in the proper place if\n",
      "the digit is 0. Although the decimal example above happens to use only 0 and 1,\n",
      "multiplication  of binary numbers must always use 0 and 1, and thus always\n",
      "offers only these two  choices. Now that we have reviewed the basics of\n",
      "multiplication, the traditional next  step is to provide the highly optimized\n",
      "multiply hardware. We break with tradition  in the belief that you will gain a\n",
      "better understanding by seeing the evolution of  the multiply hardware and\n",
      "algorithm through multiple generations. For now, let’s  assume that we are\n",
      "multiplying only positive numbers. Sequential Version of the Multiplication\n",
      "Algorithm   and Hardware This design mimics the algorithm we learned in grammar\n",
      "school; Figure 3.3 shows  the hardware.\n",
      "Page number: 265\n",
      "\n",
      "\n",
      "Score: 0.6813\n",
      "Whether the multiplicand is to be  added or not is known at the beginning of the\n",
      "multiplication by looking at each of  the 64 multiplier bits. Faster\n",
      "multiplications are possible by essentially providing  one 64-bit adder for each\n",
      "bit of the multiplier: one input is the multiplicand ANDed  with a multiplier\n",
      "bit, and the other is the output of a prior adder. A straightforward approach\n",
      "would be to connect the outputs of adders on the  right to the inputs of adders\n",
      "on the left, making a stack of adders 64 high. An  alternative way to organize\n",
      "these 64 additions is in a parallel tree, as Figure 3.7  shows. Instead of\n",
      "waiting for 64 add times, we wait just the log2 (64) or six 64-bit  add times.\n",
      "Product1 Product0 Product127 Product126 Product95..32 1 bit 1 bit 1 bit 1 bit .\n",
      ". . . . . . . . . . . . . . . . . 64 bits 64 bits 64 bits 64 bits 64 bits 64\n",
      "bits 64 bits Mplier63 • Mcand Mplier62 • Mcand Mplier61 • Mcand Mplier60 • Mcand\n",
      "Mplier3 • Mcand Mplier2 • Mcand Mplier1 • Mcand Mplier0 • Mcand FIGURE 3.7 Fast\n",
      "multiplication hardware. Rather than use a single 64-bit adder 63 times, this\n",
      "hardware “unrolls the loop” to use 63  adders and then organizes them to\n",
      "minimize delay.\n",
      "Page number: 269\n",
      "\n",
      "\n",
      "Score: 0.6799\n",
      "3.15 [10] <§3.3> Calculate the time necessary to perform a multiply using the\n",
      "approach described in the text (31 adders stacked vertically) if an integer is 8\n",
      "bits  wide and an adder takes four time units. 3.16 [20] <§3.3> Calculate the\n",
      "time necessary to perform a multiply using the  approach given in Figure 3.7 if\n",
      "an integer is 8 bits wide and an adder takes four  time units. 3.17 [20] <§3.3>\n",
      "As discussed in the text, one possible performance enhancement is  to do a shift\n",
      "and add instead of an actual multiplication. Since 9 × 6, for example, can  be\n",
      "written (2 × 2 × 2 + 1) × 6, we can calculate 9 × 6 by shifting 6 to the left\n",
      "three times  and then adding 6 to that result. Show the best way to calculate 0\n",
      "× 33 × 0 × 55 using  shifts and adds/subtracts. Assume both inputs are 8-bit\n",
      "unsigned integers. 3.18 [20] <§3.4> Using a table similar to that shown in\n",
      "Figure 3.10, calculate  74 divided by 21 using the hardware described in Figure\n",
      "3.8. You should show  the contents of each register on each step.\n",
      "Page number: 333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU memory: 6 GB\n"
     ]
    }
   ],
   "source": [
    "# Get GPU available memory\n",
    "import torch\n",
    "gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_memory_gb = round(gpu_memory_bytes / (2**30))\n",
    "print(f\"Available GPU memory: {gpu_memory_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep  8 18:32:49 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.40.06              Driver Version: 551.23         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P3             21W /  130W |    2809MiB /   6144MiB |     12%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        30      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A      4375      C   /python3.12                                 N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 6 | Recommended model: Gemma 2B in 4-bit precision.\n",
      "use_quantization_config set to: True\n",
      "model_id set to: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "if gpu_memory_gb < 5.1:\n",
    "    print(f\"Your available GPU memory is {gpu_memory_gb}GB, you may not have enough memory to run a Gemma LLM locally without quantization.\")\n",
    "elif gpu_memory_gb < 8.1:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in 4-bit precision.\")\n",
    "    use_quantization_config = True \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb < 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommended model: Gemma 2B in float16 or Gemma 7B in 4-bit precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-2b-it\"\n",
    "elif gpu_memory_gb > 19.0:\n",
    "    print(f\"GPU memory: {gpu_memory_gb} | Recommend model: Gemma 7B in 4-bit or float16 precision.\")\n",
    "    use_quantization_config = False \n",
    "    model_id = \"google/gemma-7b-it\"\n",
    "\n",
    "print(f\"use_quantization_config set to: {use_quantization_config}\")\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c879d2087c4e452a9b6824f3f0d0633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "\n",
    "if (is_flash_attn_2_available() and torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\"\n",
    "\n",
    "model_id = model_id #gemma 2b it 4bit\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, \n",
    "                                                 quantization_config = quantization_config if use_quantization_config else None,\n",
    "                                                 attn_implementation = attn_implementation)\n",
    "\n",
    "if not use_quantization_config:\n",
    "    llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1515268096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 2106740736, 'model_mem_mb': 2009.14, 'model_mem_gb': 1.96}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What is a way to efficiently implement multiplication in assembly with only addition?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "What is a way to efficiently implement multiplication in assembly with only addition?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is a way to efficiently implement multiplication in assembly with only addition?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input (tokenized):\n",
      "{'input_ids': tensor([[     2,      2,    106,   1645,    108,   1841,    603,    476,   1703,\n",
      "            577,  34790,   7133,  46824,    575,  14125,    675,   1297,   5081,\n",
      "         235336,    107,    108,    106,   2516,    108]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "\n",
      "Model output (tokens):\n",
      "tensor([     2,      2,    106,   1645,    108,   1841,    603,    476,   1703,\n",
      "           577,  34790,   7133,  46824,    575,  14125,    675,   1297,   5081,\n",
      "        235336,    107,    108,    106,   2516,    108,    688,   3887, 235248,\n",
      "        235274, 235292,  12266,    476,  10273,    688,    109,   1917,  13119,\n",
      "           108,  11246, 235298,  11635, 235292,    108,    141,  11246,    548,\n",
      "        235274, 235269,    548, 235284, 235269,    548, 235304,    139, 235289,\n",
      "         88748,    548, 235274,    578,    548, 235284, 235269,    578,   4659,\n",
      "           573,   2196,    575,    548, 235304,    108,    141,  80451,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    139, 235289,\n",
      "        122903,    548, 235274,    731, 235248, 235274,    108,    141, 235312,\n",
      "         10007, 235298,  11635,    139, 235289,  43630,    573,  46824,  10273,\n",
      "           109, 235289,  13686,  14335, 235292,    108,  11246,    548, 235274,\n",
      "        235269,    548, 235284, 235269,    548, 235304,    108,  80451,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    108,  11246,\n",
      "           548, 235274, 235269,    548, 235284, 235269,    548, 235304,    108,\n",
      "         80451,    548, 235274, 235269,    548, 235274, 235269, 235248, 235274,\n",
      "           108,  11246,    548, 235274, 235269,    548, 235284, 235269,    548,\n",
      "        235304,    108,   1917,    109,    688,   3887, 235248, 235284, 235292,\n",
      "         12266,    476,  10205,   7339,    688,    109,   1917,  13119,    108,\n",
      "         11246, 235298,  19494, 235292,    108,    141,  11246,    548, 235274,\n",
      "        235269,    548, 235284, 235269,    548, 235304,    139, 235289,  88748,\n",
      "           548, 235274,    578,    548, 235284, 235269,    578,   4659,    573,\n",
      "          2196,    575,    548, 235304,    108,    141, 235256,    529,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    139, 235289,\n",
      "         42651,    548, 235274,   1833,    731, 235248, 235274,   3298,    108,\n",
      "           141,   1254,    548, 235274, 235269,    548, 235274, 235269,    548,\n",
      "        235304,    139, 235289,   4463,    548, 235274,    577,    548, 235304,\n",
      "           577,  57841,    604,    573,  10205,    108,   1917,    109,    688,\n",
      "          3887, 235248, 235304, 235292,  12266,    476,   7339,   2482,    688,\n",
      "           109,   1917,  13119,    108,  11246, 235298,   1716, 235292,    108,\n",
      "           141, 235289,   4814,    573,  26480,    774,    476,   2482,    108,\n",
      "           141], device='cuda:0')\n",
      "\n",
      "Model output (decoded):\n",
      "tensor([     2,      2,    106,   1645,    108,   1841,    603,    476,   1703,\n",
      "           577,  34790,   7133,  46824,    575,  14125,    675,   1297,   5081,\n",
      "        235336,    107,    108,    106,   2516,    108,    688,   3887, 235248,\n",
      "        235274, 235292,  12266,    476,  10273,    688,    109,   1917,  13119,\n",
      "           108,  11246, 235298,  11635, 235292,    108,    141,  11246,    548,\n",
      "        235274, 235269,    548, 235284, 235269,    548, 235304,    139, 235289,\n",
      "         88748,    548, 235274,    578,    548, 235284, 235269,    578,   4659,\n",
      "           573,   2196,    575,    548, 235304,    108,    141,  80451,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    139, 235289,\n",
      "        122903,    548, 235274,    731, 235248, 235274,    108,    141, 235312,\n",
      "         10007, 235298,  11635,    139, 235289,  43630,    573,  46824,  10273,\n",
      "           109, 235289,  13686,  14335, 235292,    108,  11246,    548, 235274,\n",
      "        235269,    548, 235284, 235269,    548, 235304,    108,  80451,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    108,  11246,\n",
      "           548, 235274, 235269,    548, 235284, 235269,    548, 235304,    108,\n",
      "         80451,    548, 235274, 235269,    548, 235274, 235269, 235248, 235274,\n",
      "           108,  11246,    548, 235274, 235269,    548, 235284, 235269,    548,\n",
      "        235304,    108,   1917,    109,    688,   3887, 235248, 235284, 235292,\n",
      "         12266,    476,  10205,   7339,    688,    109,   1917,  13119,    108,\n",
      "         11246, 235298,  19494, 235292,    108,    141,  11246,    548, 235274,\n",
      "        235269,    548, 235284, 235269,    548, 235304,    139, 235289,  88748,\n",
      "           548, 235274,    578,    548, 235284, 235269,    578,   4659,    573,\n",
      "          2196,    575,    548, 235304,    108,    141, 235256,    529,    548,\n",
      "        235274, 235269,    548, 235274, 235269, 235248, 235274,    139, 235289,\n",
      "         42651,    548, 235274,   1833,    731, 235248, 235274,   3298,    108,\n",
      "           141,   1254,    548, 235274, 235269,    548, 235274, 235269,    548,\n",
      "        235304,    139, 235289,   4463,    548, 235274,    577,    548, 235304,\n",
      "           577,  57841,    604,    573,  10205,    108,   1917,    109,    688,\n",
      "          3887, 235248, 235304, 235292,  12266,    476,   7339,   2482,    688,\n",
      "           109,   1917,  13119,    108,  11246, 235298,   1716, 235292,    108,\n",
      "           141, 235289,   4814,    573,  26480,    774,    476,   2482,    108,\n",
      "           141], device='cuda:0')\n",
      "\n",
      "CPU times: user 11.6 s, sys: 1.63 s, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize the input text (turn it into numbers) and send it to GPU\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(f\"Model input (tokenized):\\n{input_ids}\\n\")\n",
    "\n",
    "# Generate outputs passed on the tokenized input\n",
    "# See generate docs: https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig \n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             max_new_tokens=256) # define the maximum number of new tokens to create\n",
    "print(f\"Model output (tokens):\\n{outputs[0]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output (decoded):\n",
      "<bos><bos><start_of_turn>user\n",
      "What is a way to efficiently implement multiplication in assembly with only addition?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "**Method 1: Using a loop**\n",
      "\n",
      "```assembly\n",
      "mul_loop:\n",
      "    mul r1, r2, r3  ; Multiply r1 and r2, and store the result in r3\n",
      "    addi r1, r1, 1  ; Increment r1 by 1\n",
      "    j mul_loop  ; Repeat the multiplication loop\n",
      "\n",
      "; Example usage:\n",
      "mul r1, r2, r3\n",
      "addi r1, r1, 1\n",
      "mul r1, r2, r3\n",
      "addi r1, r1, 1\n",
      "mul r1, r2, r3\n",
      "```\n",
      "\n",
      "**Method 2: Using a shift register**\n",
      "\n",
      "```assembly\n",
      "mul_shift:\n",
      "    mul r1, r2, r3  ; Multiply r1 and r2, and store the result in r3\n",
      "    sll r1, r1, 1  ; Shift r1 right by 1 bit\n",
      "    add r1, r1, r3  ; Add r1 to r3 to compensate for the shift\n",
      "```\n",
      "\n",
      "**Method 3: Using a register file**\n",
      "\n",
      "```assembly\n",
      "mul_file:\n",
      "    ; Read the coefficients from a file\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens to text\n",
    "outputs_decoded = tokenizer.decode(outputs[0])\n",
    "print(f\"Model output (decoded):\\n{outputs_decoded}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in ./env/lib/python3.12/site-packages (2.4.0)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (798.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.12/site-packages (from torch) (74.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./env/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./env/lib/python3.12/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in ./env/lib/python3.12/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./env/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "Successfully installed torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
      "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
      "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
      "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
      "Initialized empty Git repository in /home/nmbm119/370RAG/.git/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
